{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "570a262f",
   "metadata": {},
   "source": [
    "### Code for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59d19eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "\n",
    "class MRRStormDetector:\n",
    "    \"\"\"\n",
    "    Storm detection and meltingâ€“layer retrieval for MRR data,\n",
    "    including quality control filtering and 10-minute segmented\n",
    "    moving average calculation for melting layer within each event.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 gap_threshold_h: float = 3.0,\n",
    "                 detection_h_m: float = 450.0,\n",
    "                 gate_spacing_m: float = 100.0,\n",
    "                 melt_max_h_m: float = 1500.0,\n",
    "                 min_duration_h: float = 0.75,  # 45 minutes minimum\n",
    "                 min_gates_per_minute: int = 15,\n",
    "                 segment_duration_min: float = 10.0):  # 10-minute segments\n",
    "        self.gap_threshold_h = gap_threshold_h\n",
    "        self.detection_h_m = detection_h_m\n",
    "        self.gate_spacing_m = gate_spacing_m\n",
    "        self.melt_max_h_m = melt_max_h_m\n",
    "        self.min_duration_h = min_duration_h\n",
    "        self.min_gates_per_minute = min_gates_per_minute\n",
    "        self.segment_duration_min = segment_duration_min\n",
    "\n",
    "    def load_dataset(self, filepath: str) -> xr.Dataset:\n",
    "        return xr.open_dataset(filepath)\n",
    "\n",
    "    def find_detection_gate(self, ds: xr.Dataset) -> int:\n",
    "        heights = (np.arange(ds.dims[\"range\"]) + 1) * self.gate_spacing_m\n",
    "        return int(np.argmin(np.abs(heights - self.detection_h_m)))\n",
    "\n",
    "    def make_precip_mask(self, refl: xr.DataArray, gate_idx: int) -> np.ndarray:\n",
    "        gate_refl = refl.isel(range=gate_idx)\n",
    "        return (~np.isnan(gate_refl)).values\n",
    "\n",
    "    def detect_melting_layer(self,\n",
    "                             refl: xr.DataArray,\n",
    "                             vel: xr.DataArray,\n",
    "                             precip_mask: np.ndarray,\n",
    "                             gate_mask_offset: int = 2) -> np.ndarray:\n",
    "        T, G = refl.shape\n",
    "        melt_heights = np.full(T, np.nan)\n",
    "        max_gate = int(self.melt_max_h_m // self.gate_spacing_m)\n",
    "        gate_slice = slice(gate_mask_offset, max_gate)\n",
    "\n",
    "        dbz = refl.values\n",
    "        w = vel.values\n",
    "\n",
    "        for t in np.where(precip_mask)[0]:\n",
    "            z_profile = dbz[t, gate_slice]\n",
    "            w_profile = w[t, gate_slice]\n",
    "            if z_profile.size == 0 or w_profile.size == 0:\n",
    "                continue\n",
    "            if np.all(np.isnan(z_profile)) or np.all(np.isnan(w_profile)):\n",
    "                continue\n",
    "\n",
    "            peak_gate_rel = int(np.nanargmax(z_profile))\n",
    "            w_grad = np.diff(w_profile)\n",
    "            if w_grad.size == 0 or np.all(np.isnan(w_grad)):\n",
    "                continue\n",
    "            accel_gate_rel = int(np.nanargmax(w_grad))\n",
    "\n",
    "            if abs(peak_gate_rel - accel_gate_rel) <= 1:\n",
    "                best_gate_rel = int(round((peak_gate_rel + accel_gate_rel) / 2))\n",
    "            else:\n",
    "                best_gate_rel = peak_gate_rel\n",
    "\n",
    "            best_gate_abs = gate_mask_offset + best_gate_rel\n",
    "            melt_heights[t] = (best_gate_abs + 1) * self.gate_spacing_m\n",
    "\n",
    "        return melt_heights\n",
    "\n",
    "    def calculate_event_segments(self, \n",
    "                                start_time: pd.Timestamp, \n",
    "                                end_time: pd.Timestamp, \n",
    "                                segment_duration_min: float) -> List[Dict]:\n",
    "        segments = []\n",
    "        segment_delta = pd.Timedelta(minutes=segment_duration_min)\n",
    "        current_start = start_time\n",
    "        \n",
    "        while current_start < end_time:\n",
    "            current_end = min(current_start + segment_delta, end_time)\n",
    "            segments.append({\n",
    "                'segment_start': current_start,\n",
    "                'segment_end': current_end,\n",
    "                'segment_duration_min': (current_end - current_start).total_seconds() / 60.0\n",
    "            })\n",
    "            current_start = current_end\n",
    "            \n",
    "        return segments\n",
    "\n",
    "    def calculate_segment_melting_layer_averages(self,\n",
    "                                               melt_heights: np.ndarray,\n",
    "                                               times: np.ndarray,\n",
    "                                               start_idx: int,\n",
    "                                               end_idx: int) -> List[float]:\n",
    "\n",
    "        # Extract event data\n",
    "        event_melt_heights = melt_heights[start_idx:end_idx+1]\n",
    "        event_times = times[start_idx:end_idx+1]\n",
    "        times_dt = pd.to_datetime(event_times)\n",
    "        \n",
    "        # Get event start and end times\n",
    "        start_time = times_dt[0]\n",
    "        end_time = times_dt[-1]\n",
    "        \n",
    "        # Calculate segments\n",
    "        segments = self.calculate_event_segments(start_time, end_time, self.segment_duration_min)\n",
    "        \n",
    "        segment_averages = []\n",
    "        for segment in segments:\n",
    "            # Find data points within this segment\n",
    "            mask = (times_dt >= segment['segment_start']) & (times_dt <= segment['segment_end'])\n",
    "            segment_melt_data = event_melt_heights[mask]\n",
    "            \n",
    "            # Calculate average, ignoring NaN values\n",
    "            valid_data = segment_melt_data[~np.isnan(segment_melt_data)]\n",
    "            if len(valid_data) > 0:\n",
    "                segment_avg = float(np.mean(valid_data))\n",
    "            else:\n",
    "                segment_avg = np.nan\n",
    "                \n",
    "            segment_averages.append(segment_avg)\n",
    "            \n",
    "        return segment_averages\n",
    "\n",
    "    def identify_events(self, mask: np.ndarray, times: np.ndarray) -> List[Dict]:\n",
    "        times_dt = pd.to_datetime(times)\n",
    "        idxs = np.where(mask)[0]\n",
    "        if idxs.size == 0:\n",
    "            return []\n",
    "\n",
    "        events, start, end = [], idxs[0], idxs[0]\n",
    "        for i in idxs[1:]:\n",
    "            gap_h = (times_dt[i] - times_dt[end]).total_seconds() / 3600.0\n",
    "            if gap_h <= self.gap_threshold_h:\n",
    "                end = i\n",
    "            else:\n",
    "                events.append((start, end))\n",
    "                start = end = i\n",
    "        events.append((start, end))\n",
    "\n",
    "        return [\n",
    "            dict(row_start=s,\n",
    "                 row_end=e,\n",
    "                 start_time=times_dt[s],\n",
    "                 end_time=times_dt[e],\n",
    "                 duration_h=(times_dt[e] - times_dt[s]).total_seconds() / 3600.0)\n",
    "            for s, e in events\n",
    "        ]\n",
    "\n",
    "    def apply_quality_filters(self, \n",
    "                             events: List[Dict], \n",
    "                             refl_data: xr.DataArray) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Applying quality control filters to remove events with insufficient \n",
    "        duration or data availability.\n",
    "        \"\"\"\n",
    "        if not events:\n",
    "            return []\n",
    "            \n",
    "        initial_count = len(events)\n",
    "        duration_filtered = []\n",
    "        \n",
    "        # Filter 1: Duration check\n",
    "        for event in events:\n",
    "            if event[\"duration_h\"] >= self.min_duration_h:\n",
    "                duration_filtered.append(event)\n",
    "        \n",
    "        duration_removed = initial_count - len(duration_filtered)\n",
    "        \n",
    "        # Filter 2: Data availability check\n",
    "        data_filtered = []\n",
    "        for event in duration_filtered:\n",
    "            s, e = event[\"row_start\"], event[\"row_end\"]\n",
    "            event_data = refl_data.isel(time=slice(s, e+1))\n",
    "            \n",
    "            has_sufficient_data = False\n",
    "            for t in range(event_data.shape[0]):\n",
    "                valid_gates = np.sum(~np.isnan(event_data.values[t, :]))\n",
    "                if valid_gates >= self.min_gates_per_minute:\n",
    "                    has_sufficient_data = True\n",
    "                    break\n",
    "                    \n",
    "            if has_sufficient_data:\n",
    "                data_filtered.append(event)\n",
    "        \n",
    "        data_removed = len(duration_filtered) - len(data_filtered)\n",
    "        total_removed = initial_count - len(data_filtered)\n",
    "        \n",
    "        print(f\"Quality control filtering results:\")\n",
    "        print(f\"  Initial events: {initial_count}\")\n",
    "        print(f\"  Removed by duration filter (<{self.min_duration_h:.2f}h): {duration_removed}\")\n",
    "        print(f\"  Removed by data availability filter (<{self.min_gates_per_minute} gates): {data_removed}\")\n",
    "        print(f\"  Final events after filtering: {len(data_filtered)}\")\n",
    "        print(f\"  Total removal rate: {total_removed/initial_count*100:.1f}%\")\n",
    "            \n",
    "        return data_filtered\n",
    "\n",
    "    def classify_events(self,\n",
    "                        ds: xr.Dataset,\n",
    "                        events: List[Dict],\n",
    "                        refl_var: str,\n",
    "                        melt_heights: np.ndarray) -> List[Dict]:\n",
    "        enhanced = []\n",
    "        times = ds.time.values\n",
    "        \n",
    "        for i, ev in enumerate(events, 1):\n",
    "            s, e = ev[\"row_start\"], ev[\"row_end\"]\n",
    "            seg = ds[refl_var].isel(time=slice(s, e+1))\n",
    "            mean_ = float(seg.mean().item())\n",
    "            max_ = float(seg.max().item())\n",
    "            \n",
    "            med_m = float(np.nanmedian(melt_heights[s:e+1]))\n",
    "            \n",
    "            # Calculating 10-minute segment averages for this event\n",
    "            segment_averages = self.calculate_segment_melting_layer_averages(\n",
    "                melt_heights, times, s, e\n",
    "            )\n",
    "            \n",
    "            # Calculating number of 10-minute segments\n",
    "            num_segments = len(segment_averages)\n",
    "\n",
    "            # Keeping existing classification logic unchanged\n",
    "            if max_ > 30 and ev[\"duration_h\"] < 4:\n",
    "                stype = \"convective\"\n",
    "            elif ev[\"duration_h\"] > 8 and max_ < 25:\n",
    "                stype = \"stratiform\"\n",
    "            else:\n",
    "                stype = \"mixed\"\n",
    "\n",
    "            enhanced.append({**ev,\n",
    "                             \"storm_id\": i,\n",
    "                             \"mean_dbz\": round(mean_, 2),\n",
    "                             \"max_dbz\": round(max_, 2),\n",
    "                             \"storm_type\": stype,\n",
    "                             \"median_melting_layer_m\": round(med_m, 1),\n",
    "                             \"num_10min_segments\": num_segments,\n",
    "                             \"segment_ml_averages\": [round(avg, 1) if not np.isnan(avg) else None \n",
    "                                                   for avg in segment_averages]})\n",
    "        return enhanced\n",
    "\n",
    "    def run(self,\n",
    "            filepath: str,\n",
    "            refl_var: str = \"Ze\",\n",
    "            vel_var: str = \"W\",\n",
    "            time_coord: str = \"time\") -> List[Dict]:\n",
    "        print(f\"Loading dataset: {filepath}\")\n",
    "        ds = self.load_dataset(filepath)\n",
    "        for var in (refl_var, vel_var):\n",
    "            if var not in ds:\n",
    "                raise KeyError(f\"Variable '{var}' not found in file.\")\n",
    "\n",
    "        print(\"Detecting precipitation and melting layers...\")\n",
    "        gate_idx = self.find_detection_gate(ds)\n",
    "        precip_mask = self.make_precip_mask(ds[refl_var], gate_idx)\n",
    "        melt_ht = self.detect_melting_layer(ds[refl_var],\n",
    "                                           ds[vel_var],\n",
    "                                           precip_mask,\n",
    "                                           gate_mask_offset=2)\n",
    "        \n",
    "        print(\"Identifying precipitation events...\")\n",
    "        events = self.identify_events(precip_mask, ds[time_coord].values)\n",
    "        \n",
    "        print(f\"Applying quality control filters...\")\n",
    "        filtered_events = self.apply_quality_filters(events, ds[refl_var])\n",
    "        \n",
    "        print(\"Classifying storm events with 10-minute segment analysis...\")\n",
    "        storms = self.classify_events(ds, filtered_events, refl_var, melt_ht)\n",
    "        return storms\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    detector = MRRStormDetector(\n",
    "        gap_threshold_h=3.0,\n",
    "        detection_h_m=450.0,\n",
    "        gate_spacing_m=100.0,\n",
    "        melt_max_h_m=1500.0,\n",
    "        min_duration_h=0.75,  # 45 minutes\n",
    "        min_gates_per_minute=15,\n",
    "        segment_duration_min=10.0  # 10-minute segments\n",
    "    )\n",
    "\n",
    "    storms = detector.run(\n",
    "        filepath=\"C:\\\\Working Folder\\\\Complete_Data\\\\merged_output_new.nc\",\n",
    "        refl_var=\"Ze\",\n",
    "        vel_var=\"W\",\n",
    "        time_coord=\"time\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nFinal results - Detected {len(storms)} quality-controlled storms:\")\n",
    "    for s in storms:\n",
    "        print(f\"  â€¢ Storm {s['storm_id']:2d}: {s['start_time']} â†’ {s['end_time']}, \"\n",
    "              f\"d={s['duration_h']:.1f} h, type={s['storm_type']}, \"\n",
    "              f\"median ML={s['median_melting_layer_m']:.0f} m\")\n",
    "        print(f\"    10-min segments ({s['num_10min_segments']}): {s['segment_ml_averages']}\")\n",
    "\n",
    "    # Creating detailed output with expanded segment data\n",
    "    output_data = []\n",
    "    for storm in storms:\n",
    "        base_data = {k: v for k, v in storm.items() if k != 'segment_ml_averages'}\n",
    "        \n",
    "        # Adding each segment as a separate column\n",
    "        for i, avg in enumerate(storm['segment_ml_averages'], 1):\n",
    "            base_data[f'segment_{i}_ml_avg'] = avg\n",
    "            \n",
    "        output_data.append(base_data)\n",
    "\n",
    "    output_file = r\"C:\\Working Folder\\Summer_Work\\MRR\\new_data_storm_summary_10min_segments.csv\"\n",
    "    pd.DataFrame(output_data).to_csv(output_file, index=False)\n",
    "    print(f\"Saved 10-minute segment analysis â†’ {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
